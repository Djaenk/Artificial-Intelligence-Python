{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab Assignment Seven: Recurrent Network Architectures\n",
    "\n",
    "CS 5324\n",
    "\n",
    "2021-05-12\n",
    "\n",
    "Anthony Wang"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preparation\n",
    "\n",
    "[The dataset](https://www.kaggle.com/hetulmehta/website-classification) is a list of websites that have been scraped for text and classified into one of sixteen categories based on their content. For training a recurrent network, the length of each site's text will be homogenized to 100 words via clipping and padding. This length errs on the side of clipping, as despite the average text content of a site being over 700 words, that figure is heavily skewed by large outliers. Most sites are expected to contain at least 100 words, which should be sufficient to determined the purpose of the website. The labels are one-hot encoded for model training and classification."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6bda23abe0a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "from tensorflow import keras\n",
    "from keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "dataframe = read_csv(\"website_classification.csv\")\n",
    "print(dataframe.Category.unique())\n",
    "\n",
    "dataframe.drop(['Unnamed: 0', 'website_url'], axis=1, inplace=True)\n",
    "dataframe[\"Category\"] = keras.utils.to_categorical(dataframe[\"Category\"])\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=100)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "dataframe.sample(n=5)"
   ]
  },
  {
   "source": [
    "F-score will be used to determine performance. Uses of a site-classifier are unlikely to heavily prefer precision or recall over the other. The data is imbalanced, making accuracy unsuitable for evaluation as classification performance of a more frequent class would be prioritized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://aakashgoel12.medium.com/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "# Definition of custom f-score function\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def F1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "source": [
    "This dataset contains just 1408 instances which. This is not a large amount, and holdout would further reduce the quantity of valuable training data. Usage of data will be maximized with stratified 10-fold cross validation. This also ensures training is independent of any subset of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "X_num = df[numerical].values\n",
    "y = df[\"RainTomorrow\"].values"
   ]
  },
  {
   "source": [
    "## Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Utility function for plotting model statistics\n",
    "def plot_histories(histories):\n",
    "    F1 = []\n",
    "    val_F1 = []\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for h in histories:\n",
    "        F1.extend(h.history['F1'])\n",
    "        val_F1.extend(h.history['val_F1'])\n",
    "        loss.extend(h.history['loss'])\n",
    "        val_loss.extend(h.history['val_loss'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.suptitle(f'Fold {i+1}')\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(F1)\n",
    "\n",
    "    plt.ylabel('F-score')\n",
    "    plt.title('Training')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(val_F1)\n",
    "    plt.title('Validation')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(loss)\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(val_loss)\n",
    "    plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove.6B.50d.txt\") as f: # Assumes the 50-dimensional glove embedding file is in the local directory\r\n",
    "    for line in f:\r\n",
    "        word, coefs = line.split(maxsplit=1)\r\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
    "        embeddings_index[word] = coefs\r\n",
    "\r\n"
   ]
  }
 ]
}